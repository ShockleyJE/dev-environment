# - name: Debug - Testing llama task inclusion
#   debug:
#     msg: "Llama task file is being included successfully!"
#   tags:
#     - install
#     - llama

# - name: Create custom llama.cpp with CUDA support
#   shell: |
#     cat > ~/llama-cpp-cuda.nix << 'EOF'
#     { pkgs ? import <nixpkgs> {} }:
    
#     pkgs.llama-cpp.override {
#       enableCuda = true;
#       cudaSupport = true;
#     }
#     EOF
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama

# - name: Install llama.cpp with CUDA support using Nix
#   shell: |
#     nix-env -if ~/llama-cpp-cuda.nix
#   args:
#     creates: "{{ ansible_env.HOME }}/.nix-profile/bin/llama"
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama

# - name: Create llama directory structure
#   file:
#     path: "{{ item }}"
#     state: directory
#     mode: '0755'
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   with_items:
#     - "{{ ansible_env.HOME }}/llama"
#     - "{{ ansible_env.HOME }}/llama/models"
#     - "{{ ansible_env.HOME }}/llama/config"
#     - "{{ ansible_env.HOME }}/llama/scripts"
#     - "{{ ansible_env.HOME }}/llama/cache"
#   tags:
#     - install
#     - llama

# - name: Add llama alias to zsh configuration
#   lineinfile:
#     path: "{{ ansible_env.HOME }}/.zshrc"
#     line: 'alias lla="llama"'
#     state: present
#     create: yes
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama

# - name: Add LLAMA_MODELS environment variable to zsh configuration
#   lineinfile:
#     path: "{{ ansible_env.HOME }}/.zshrc"
#     line: 'export LLAMA_MODELS="{{ ansible_env.HOME }}/llama/models"'
#     state: present
#     create: yes
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama

# - name: Add LLAMA_CONFIG environment variable to zsh configuration
#   lineinfile:
#     path: "{{ ansible_env.HOME }}/.zshrc"
#     line: 'export LLAMA_CONFIG="{{ ansible_env.HOME }}/llama/config"'
#     state: present
#     create: yes
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama

# - name: Create README for llama directory
#   copy:
#     dest: "{{ ansible_env.HOME }}/llama/README.md"
#     content: |
#       # Llama.cpp Directory Structure
      
#       This directory contains all llama.cpp related files and configurations.
      
#       ## Directory Structure
      
#       - `models/` - Store your GGUF model files here
#       - `config/` - Configuration files, grammars, and settings
#       - `scripts/` - Custom scripts and utilities
#       - `cache/` - Temporary files and downloads
      
#       ## Usage
      
#       Use the `lla` alias to run llama.cpp commands:
#       ```bash
#       lla -m models/your-model.gguf -p "Your prompt here"
#       ```
      
#       ## Environment Variables
      
#       - `LLAMA_MODELS`: Points to the models directory
#       - `LLAMA_CONFIG`: Points to the config directory
      
#       ## GPU Support
      
#       This installation includes CUDA support for NVIDIA GPUs.
#       Use the `--gpu-layers` parameter to enable GPU acceleration:
#       ```bash
#       lla -m models/your-model.gguf -p "Your prompt here" --gpu-layers 35
#       ```
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama

# - name: Verify llama.cpp GPU support
#   shell: |
#     llama --help | grep -i gpu || echo "GPU support not detected"
#   register: llama_gpu_check
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama

# - name: Display GPU support status
#   debug:
#     msg: "{{ llama_gpu_check.stdout }}"
#   become: false
#   when: ansible_facts.distribution == "Ubuntu"
#   tags:
#     - install
#     - llama 